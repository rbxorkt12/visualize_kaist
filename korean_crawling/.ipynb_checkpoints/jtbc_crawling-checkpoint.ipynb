{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jtbc_crawling(start_date, end_date):\n",
    "    url1 = []\n",
    "    # url 변화하는거 적당히 보고 알아서 잘 해야됨. 페이지 단위로 넘어가는게 아니라면 selenium 써야할 수도 있음.\n",
    "    url_head = \"http://jtbc.joins.com/search/news?page=\"\n",
    "    url_tail = \"&term=kaist\"\n",
    "    for page in tqdm(range(1,19)):\n",
    "        url_final = url_head + str(page) + url_tail\n",
    "        response = requests.get(url_final)\n",
    "        html = response.content\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        elements = soup.select('.prg_ttl a[href]') # F12 눌러서 적당히 우리가 원하는 a href 태그 url 링크 달려있는 id 또는 class 찾아서 넣어주면 됨. \n",
    "        for element in elements:\n",
    "            link = element.attrs['href']\n",
    "            url1.append(link)\n",
    "    url2 = []\n",
    "    # url 변화하는거 적당히 보고 알아서 잘 해야됨. 페이지 단위로 넘어가는게 아니라면 selenium 써야할 수도 있음.\n",
    "    url_head = \"http://jtbc.joins.com/search/news?page=\"\n",
    "    url_tail = \"&term=카이스트\"\n",
    "    for page in tqdm(range(1,100)):\n",
    "        url_final = url_head + str(page) + url_tail\n",
    "        response = requests.get(url_final)\n",
    "        html = response.content\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        elements = soup.select('.prg_ttl a[href]') # F12 눌러서 적당히 우리가 원하는 a href 태그 url 링크 달려있는 id 또는 class 찾아서 넣어주면 됨. \n",
    "        for element in elements:\n",
    "            link = element.attrs['href']\n",
    "            url2.append(link)\n",
    "    url = list(set(url1).union(set(url2)))\n",
    "    df = pd.DataFrame({\"url\": url}, columns=[\"url\"])\n",
    "    contents = []\n",
    "    titles = []\n",
    "    for row in tqdm(df.itertuples()):\n",
    "        link = getattr(row, 'url')\n",
    "        response = requests.get(link)\n",
    "        html = response.text\n",
    "        soup = bs(html,'html.parser')\n",
    "        try:\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose() \n",
    "            date = soup.select('.i_date')[0].get_text()\n",
    "            date = date.split(' ')[1]\n",
    "            if start_date <= date and date <= end_date:\n",
    "                content = soup.select('#articlebody')[0].get_text() # select 안에 본문에 해당하는 id 값을 찾아서 넣어주면 됨.\n",
    "                content = content.replace('\\xa0', \" \").replace('\\u3000','').replace('\\r','').replace('\\n', \" \")\n",
    "                contents.append(content)\n",
    "                title = soup.select('#jtbcBody h3')[0].get_text()\n",
    "                title = title.replace('\\xa0', \" \")\n",
    "                title = title.replace('\\n', \" \")\n",
    "                titles.append(title)\n",
    "        except:\n",
    "            print(row)\n",
    "    df = pd.DataFrame({\"url\": urls, \"title\": titles, \"content\": contents}, columns=[\"url\", \"title\", \"content\"])\n",
    "    df.to_excel(\"articles/\"+str(start_date)+'-'+str(end_date)+\"_jtbc.xlsx\",index = False)\n",
    "    df.to_pickle(\"articles/\"+str(start_date)+'-'+str(end_date)+\"_jtbc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtbc_crawling('2015.01.01', '2020.10.23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:04<00:00,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 검색 결과에 따른 url 링크 저장\n",
    "url1 = []\n",
    "# url 변화하는거 적당히 보고 알아서 잘 해야됨. 페이지 단위로 넘어가는게 아니라면 selenium 써야할 수도 있음.\n",
    "url_head = \"http://jtbc.joins.com/search/news?page=\"\n",
    "url_tail = \"&term=kaist\"\n",
    "for page in tqdm(range(1,19)):\n",
    "    url_final = url_head + str(page) + url_tail\n",
    "    response = requests.get(url_final)\n",
    "    html = response.content\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    elements = soup.select('.prg_ttl a[href]') # F12 눌러서 적당히 우리가 원하는 a href 태그 url 링크 달려있는 id 또는 class 찾아서 넣어주면 됨. \n",
    "    for element in elements:\n",
    "        link = element.attrs['href']\n",
    "        url1.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 99/99 [00:25<00:00,  3.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# 검색 결과에 따른 url 링크 저장\n",
    "url2 = []\n",
    "\n",
    "# url 변화하는거 적당히 보고 알아서 잘 해야됨. 페이지 단위로 넘어가는게 아니라면 selenium 써야할 수도 있음.\n",
    "url_head = \"http://jtbc.joins.com/search/news?page=\"\n",
    "url_tail = \"&term=카이스트\"\n",
    "for page in tqdm(range(1,100)):\n",
    "    url_final = url_head + str(page) + url_tail\n",
    "    response = requests.get(url_final)\n",
    "    html = response.content\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    elements = soup.select('.prg_ttl a[href]') # F12 눌러서 적당히 우리가 원하는 a href 태그 url 링크 달려있는 id 또는 class 찾아서 넣어주면 됨. \n",
    "    for element in elements:\n",
    "        link = element.attrs['href']\n",
    "        url2.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109\n"
     ]
    }
   ],
   "source": [
    "url = list(set(url1).union(set(url2)))\n",
    "df = pd.DataFrame({\"url\": url}, columns=[\"url\"])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1109it [01:58,  9.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>titles</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>'뇌섹남' 신재평, 강한 승부욕 \"미치도록 풀고 싶다\"</td>\n",
       "      <td>페퍼톤스 신재평이 남다른 스펙을 공개해 화제다.지난 25일 방송된 tvN '뇌섹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>'뇌섹남' 윤소희, 미모에 성적까지 완벽 '진정한 엄친딸'</td>\n",
       "      <td>배우 윤소희가 화려한 학창시절 성적표를 공개해 화제다.지난 1일 방송된 tvN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>3월 16일 (수) 뉴스현장 다시보기</td>\n",
       "      <td>· [중계]유승민 공천 여부 결정 안돼…잇따르는 탈락자 반발· [토크] 유승민 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>스트레스↓ 집중도↑… 옴니핏 마인드케어 아리랑 TV서 공개</td>\n",
       "      <td>아리랑 TV의 MONEY MONSTER에서 옴니씨앤에스가 전파를 탔다. 이날 방...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>[현장IS] 두꺼운 외투없이 뛰어온 고 김주혁 유족..6시간 논의 끝에 부검 결정</td>\n",
       "      <td>고(故) 김주혁의 정확한 사인을 알기 위해 부검을 진행하기로 결정했다.김주혁의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>[포토]유승옥, 한껏 뽐내는 육감몸매</td>\n",
       "      <td>24일 오후 서울 엘루이호텔에서 열린 IT 제품 '터치플레이'의 시연 및 신제품...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>장동민 \"'더 지니어스', 또 출연 하고싶다\"</td>\n",
       "      <td>'더 지니어스' 우승자 장동민이 프로그램에 다시 출연할 의사를 밝혔다.장동민은 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>스마트농법 통해 수확량 40배…20대 농부들의 기적</td>\n",
       "      <td>[앵커]농사는 따분하지 않습니다. 수확량이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>'더 지니어스' 장동민 \"운 좋게 우승…과분한 결과다\"</td>\n",
       "      <td>\"운 좋게 우승한거죠.\"개그맨 장동민이 '더 지니어스' 우승 소감을 전했다.장동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://news.jtbc.joins.com/article/article.asp...</td>\n",
       "      <td>게임회사 웹젠 김병관 의장 더민주 입당…문재인 영입 2호</td>\n",
       "      <td>게임회사 웹젠의 김병관(43) 이사회 의장이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://news.jtbc.joins.com/article/article.asp...   \n",
       "1  http://news.jtbc.joins.com/article/article.asp...   \n",
       "2  http://news.jtbc.joins.com/article/article.asp...   \n",
       "3  http://news.jtbc.joins.com/article/article.asp...   \n",
       "4  http://news.jtbc.joins.com/article/article.asp...   \n",
       "5  http://news.jtbc.joins.com/article/article.asp...   \n",
       "6  http://news.jtbc.joins.com/article/article.asp...   \n",
       "7  http://news.jtbc.joins.com/article/article.asp...   \n",
       "8  http://news.jtbc.joins.com/article/article.asp...   \n",
       "9  http://news.jtbc.joins.com/article/article.asp...   \n",
       "\n",
       "                                          titles  \\\n",
       "0                 '뇌섹남' 신재평, 강한 승부욕 \"미치도록 풀고 싶다\"   \n",
       "1               '뇌섹남' 윤소희, 미모에 성적까지 완벽 '진정한 엄친딸'   \n",
       "2                           3월 16일 (수) 뉴스현장 다시보기   \n",
       "3               스트레스↓ 집중도↑… 옴니핏 마인드케어 아리랑 TV서 공개   \n",
       "4  [현장IS] 두꺼운 외투없이 뛰어온 고 김주혁 유족..6시간 논의 끝에 부검 결정   \n",
       "5                           [포토]유승옥, 한껏 뽐내는 육감몸매   \n",
       "6                      장동민 \"'더 지니어스', 또 출연 하고싶다\"   \n",
       "7                   스마트농법 통해 수확량 40배…20대 농부들의 기적   \n",
       "8                 '더 지니어스' 장동민 \"운 좋게 우승…과분한 결과다\"   \n",
       "9                게임회사 웹젠 김병관 의장 더민주 입당…문재인 영입 2호   \n",
       "\n",
       "                                            contents  \n",
       "0    페퍼톤스 신재평이 남다른 스펙을 공개해 화제다.지난 25일 방송된 tvN '뇌섹...  \n",
       "1    배우 윤소희가 화려한 학창시절 성적표를 공개해 화제다.지난 1일 방송된 tvN ...  \n",
       "2    · [중계]유승민 공천 여부 결정 안돼…잇따르는 탈락자 반발· [토크] 유승민 ...  \n",
       "3    아리랑 TV의 MONEY MONSTER에서 옴니씨앤에스가 전파를 탔다. 이날 방...  \n",
       "4    고(故) 김주혁의 정확한 사인을 알기 위해 부검을 진행하기로 결정했다.김주혁의 ...  \n",
       "5    24일 오후 서울 엘루이호텔에서 열린 IT 제품 '터치플레이'의 시연 및 신제품...  \n",
       "6    '더 지니어스' 우승자 장동민이 프로그램에 다시 출연할 의사를 밝혔다.장동민은 ...  \n",
       "7                        [앵커]농사는 따분하지 않습니다. 수확량이 ...  \n",
       "8    \"운 좋게 우승한거죠.\"개그맨 장동민이 '더 지니어스' 우승 소감을 전했다.장동...  \n",
       "9                        게임회사 웹젠의 김병관(43) 이사회 의장이...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기사 내용 긁어오기\n",
    "contents = []\n",
    "titles = []\n",
    "for row in tqdm(df.itertuples()):\n",
    "    link = getattr(row, 'url')\n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    soup = bs(html,'html.parser')\n",
    "    content = soup.select('#articlebody')[0].get_text() # select 안에 본문에 해당하는 id 값을 찾아서 넣어주면 됨.\n",
    "    content = content.replace('\\xa0', \" \")\n",
    "    content = content.replace('\\n', \" \")\n",
    "    contents.append(content)\n",
    "    title = soup.select('#jtbcBody h3')[0].get_text()\n",
    "    title = title.replace('\\xa0', \" \")\n",
    "    title = title.replace('\\n', \" \")\n",
    "    titles.append(title)\n",
    "df['titles'] = titles\n",
    "df['contents'] = contents\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"articles/jtbc.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"pickle files/jtbc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
